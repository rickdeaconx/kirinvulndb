"""
Automated Vulnerability Discovery System
Monitors multiple sources every 6 hours for new AI coding tool vulnerabilities
Comprehensive coverage of all reasonable vulnerability sources
"""

import asyncio
import aiohttp
import logging
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from dataclasses import dataclass
import feedparser
import xml.etree.ElementTree as ET

from app.db.database import SessionLocal
from app.models.vulnerability import Vulnerability, SeverityEnum, PatchStatusEnum, AttackVectorEnum
from app.models.tool import AITool
from app.core.config import settings

logger = logging.getLogger(__name__)

@dataclass
class VulnSource:
    name: str
    url: str
    source_type: str  # "nvd_api", "github_api", "rss", "json_api"
    keywords: List[str]
    check_interval: int  # hours

class VulnerabilityMonitor:
    """Main vulnerability monitoring system"""
    
    def __init__(self):
        self.session = None
        self.ai_keywords = [
            # Cursor gets TOP priority - our primary AI IDE
            "cursor", "cursor ide", "cursor.sh", "cursor editor", "cursor ai",
            "copilot", "tabnine", "codeium", "code completion", 
            "ai assistant", "code generation", "machine learning", "artificial intelligence",
            "jetbrains ai", "amazon codewhisperer", "sourcegraph cody", "replit ghostwriter",
            "vscode extension", "ide plugin", "code suggestion", "autocomplete",
            "llm", "large language model", "prompt injection", "ai security",
            "neural code", "transformer model", "deepcode", "ai code review",
            "pair programming", "ai pair", "github copilot", "intellicode",
            "ai-powered", "ai-assisted", "model inference", "code synthesis"
        ]
        
        # Define all vulnerability sources
        self.sources = [
            # NVD API for CVEs
            VulnSource(
                name="NVD_API",
                url="https://services.nvd.nist.gov/rest/json/cves/2.0",
                source_type="nvd_api", 
                keywords=self.ai_keywords,
                check_interval=24
            ),
            
            # GitHub Security Advisories
            VulnSource(
                name="GITHUB_API",
                url="https://api.github.com/advisories",
                source_type="github_api",
                keywords=self.ai_keywords,
                check_interval=12
            ),
            
            # Security RSS Feeds
            VulnSource(
                name="CERT_RSS",
                url="https://www.cisa.gov/cybersecurity-advisories/all.xml",
                source_type="rss",
                keywords=self.ai_keywords,
                check_interval=24
            ),
            
            VulnSource(
                name="SECURITY_FOCUS_RSS", 
                url="https://www.securityfocus.com/rss/vulnerabilities.xml",
                source_type="rss",
                keywords=self.ai_keywords,
                check_interval=24
            ),
            
            # Vendor-specific sources
            VulnSource(
                name="MICROSOFT_SECURITY",
                url="https://api.msrc.microsoft.com/cvrf/v2.0/updates",
                source_type="json_api",
                keywords=["vscode", "copilot", "visual studio"],
                check_interval=24
            ),
            
            VulnSource(
                name="GITHUB_SECURITY_BLOG",
                url="https://github.blog/category/security/feed/",
                source_type="rss", 
                keywords=["copilot", "github", "security"],
                check_interval=24
            ),
            
            VulnSource(
                name="JETBRAINS_SECURITY",
                url="https://blog.jetbrains.com/security/feed/",
                source_type="rss",
                keywords=["security", "vulnerability", "intellij", "ide"],
                check_interval=24
            ),
            
            # Additional sources based on authenticated findings
            VulnSource(
                name="CURSOR_SECURITY_RESEARCH",
                url="https://cursor.sh/security",
                source_type="web_scrape",
                keywords=["cursor", "vulnerability", "security advisory", "CVE"],
                check_interval=12
            ),
            
            VulnSource(
                name="KASPERSKY_SECURELIST", 
                url="https://securelist.com/feed/",
                source_type="rss",
                keywords=["cursor", "ide", "extension", "malware", "cryptocurrency"],
                check_interval=24
            ),
            
            VulnSource(
                name="SECURITY_RESEARCH_BLOGS",
                url="https://blog.checkpoint.com/feed/",
                source_type="rss", 
                keywords=["cursor", "mcp", "prompt injection", "ai security"],
                check_interval=24
            ),
            
            VulnSource(
                name="IMPERVA_RESEARCH",
                url="https://www.imperva.com/blog/feed/",
                source_type="rss",
                keywords=["cursor", "workspace trust", "ide security", "vscode"],
                check_interval=24
            ),
            
            VulnSource(
                name="HIDDENLAYER_RESEARCH", 
                url="https://hiddenlayer.com/research/feed/",
                source_type="rss",
                keywords=["prompt injection", "ai security", "cursor", "github"],
                check_interval=24
            ),
            
            # Community and forum sources (simulated - would need special handling)
            VulnSource(
                name="SECURITY_COMMUNITY_FEEDS",
                url="https://reddit.com/r/netsec/.rss",
                source_type="rss",
                keywords=["cursor", "ide vulnerability", "ai security", "prompt injection"],
                check_interval=12
            ),
            
            # Additional comprehensive sources for maximum coverage
            VulnSource(
                name="MITRE_CVE_FEED",
                url="https://cve.mitre.org/data/downloads/allitems.csv",
                source_type="csv_download",
                keywords=self.ai_keywords,
                check_interval=12
            ),
            
            VulnSource(
                name="EXPLOIT_DB_RSS",
                url="https://www.exploit-db.com/rss.xml",
                source_type="rss",
                keywords=["ide", "editor", "ai", "copilot", "cursor"],
                check_interval=6
            ),
            
            VulnSource(
                name="VULNERS_SEARCH",
                url="https://vulners.com/api/v3/search/lucene/",
                source_type="json_api",
                keywords=["cursor", "copilot", "tabnine", "codeium", "vscode"],
                check_interval=12
            ),
            
            VulnSource(
                name="OPENCVE_RSS",
                url="https://www.opencve.io/cve/rss",
                source_type="rss",
                keywords=self.ai_keywords,
                check_interval=8
            ),
            
            VulnSource(
                name="BLEEPING_COMPUTER",
                url="https://www.bleepingcomputer.com/feed/",
                source_type="rss",
                keywords=["vscode", "cursor", "ide", "ai assistant", "copilot"],
                check_interval=6
            ),
            
            VulnSource(
                name="THREATPOST_RSS",
                url="https://threatpost.com/feed/",
                source_type="rss",
                keywords=["ai security", "ide", "editor", "development tool"],
                check_interval=8
            ),
            
            VulnSource(
                name="DARK_READING",
                url="https://www.darkreading.com/rss_simple.asp",
                source_type="rss",
                keywords=["artificial intelligence", "ai", "development", "ide"],
                check_interval=12
            ),
            
            VulnSource(
                name="SECURITY_WEEK",
                url="https://www.securityweek.com/feed/",
                source_type="rss", 
                keywords=["ai", "development tool", "code", "editor"],
                check_interval=8
            ),
            
            VulnSource(
                name="HACKER_NEWS_SECURITY",
                url="https://feeds.feedburner.com/oreilly/radar",
                source_type="rss",
                keywords=["security", "ai", "development", "vulnerability"],
                check_interval=6
            ),
            
            VulnSource(
                name="RAPID7_BLOG",
                url="https://www.rapid7.com/blog/rss/",
                source_type="rss",
                keywords=["ai", "development", "vulnerability", "security"],
                check_interval=12
            ),
            
            VulnSource(
                name="SOPHOS_NAKED_SECURITY",
                url="https://nakedsecurity.sophos.com/feed/",
                source_type="rss",
                keywords=["ai", "development tool", "editor", "ide"],
                check_interval=8
            ),
            
            VulnSource(
                name="QUALYS_BLOG",
                url="https://blog.qualys.com/feed",
                source_type="rss",
                keywords=["ai", "development", "vulnerability"],
                check_interval=12
            ),
            
            # Vendor-specific additional sources
            VulnSource(
                name="OPENAI_SECURITY",
                url="https://openai.com/security/feed/",
                source_type="rss",
                keywords=["security", "vulnerability", "disclosure"],
                check_interval=8
            ),
            
            VulnSource(
                name="ANTHROPIC_SECURITY",
                url="https://www.anthropic.com/security/feed/",
                source_type="rss",
                keywords=["security", "vulnerability", "ai safety"],
                check_interval=8
            ),
            
            VulnSource(
                name="GOOGLE_AI_SECURITY",
                url="https://blog.google/technology/ai/feed/",
                source_type="rss",
                keywords=["security", "vulnerability", "ai"],
                check_interval=12
            ),
            
            VulnSource(
                name="MICROSOFT_SECURITY_BLOG",
                url="https://www.microsoft.com/security/blog/feed/",
                source_type="rss",
                keywords=["copilot", "vscode", "ai", "security"],
                check_interval=8
            ),
            
            # Academic and research sources
            VulnSource(
                name="ARXIV_CS_SECURITY",
                url="http://export.arxiv.org/rss/cs.CR",
                source_type="rss",
                keywords=["ai security", "prompt injection", "adversarial"],
                check_interval=24
            ),
            
            VulnSource(
                name="IEEE_SECURITY",
                url="https://www.computer.org/csdl/magazine/sp/rss.xml",
                source_type="rss", 
                keywords=["ai", "machine learning", "security"],
                check_interval=24
            ),
            
            # Specialized AI security sources
            VulnSource(
                name="AI_SECURITY_RESEARCH",
                url="https://blog.trailofbits.com/feed/",
                source_type="rss",
                keywords=["ai", "llm", "security", "vulnerability"],
                check_interval=8
            ),
            
            VulnSource(
                name="OWASP_AI_SECURITY",
                url="https://owasp.org/www-project-ai-security-and-privacy-guide/feed.xml",
                source_type="rss",
                keywords=["ai security", "llm", "ml security"],
                check_interval=12
            )
        ]

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Kirin-VulnDB/1.0 (Security Research)'}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def check_nvd_api(self, source: VulnSource) -> List[Dict]:
        """Check NVD API for new CVEs - requires valid API key"""
        logger.info("Checking NVD API for new vulnerabilities...")
        vulnerabilities = []
        
        # Check if we have valid API credentials
        api_key = settings.NVD_API_KEY
        if not api_key or api_key in ['your_nvd_api_key', 'placeholder', '']:
            logger.warning("No valid NVD API key configured - skipping NVD collection")
            logger.info("To enable NVD collection, set NVD_API_KEY in your .env file")
            return vulnerabilities
        
        try:
            # Check for CVEs from last 30 days
            start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%dT%H:%M:%S.000")
            end_date = datetime.now().strftime("%Y-%m-%dT%H:%M:%S.000")
            
            headers = {
                "apiKey": api_key,
                "User-Agent": "Kirin-VulnDB/1.0"
            }
            
            for keyword in source.keywords:
                params = {
                    "keywordSearch": keyword,
                    "lastModStartDate": start_date,
                    "lastModEndDate": end_date,
                    "resultsPerPage": 20
                }
                
                async with self.session.get(source.url, params=params, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        cve_items = data.get("vulnerabilities", [])
                        
                        for item in cve_items:
                            vuln_data = await self.parse_nvd_vulnerability(item)
                            if vuln_data and await self.is_ai_related(vuln_data):
                                vulnerabilities.append(vuln_data)
                        
                        logger.info(f"Found {len(cve_items)} CVEs for keyword '{keyword}'")
                    elif response.status == 403:
                        logger.error("NVD API key invalid or rate limit exceeded")
                        break
                    else:
                        logger.warning(f"NVD API returned {response.status}: {await response.text()}")
                
                # Rate limiting - NVD requires delays between requests
                await asyncio.sleep(2)
                
        except Exception as e:
            logger.error(f"Error checking NVD API: {e}")
        
        return vulnerabilities

    async def check_github_advisories(self, source: VulnSource) -> List[Dict]:
        """Check GitHub Security Advisories"""
        logger.info("Checking GitHub Security Advisories...")
        vulnerabilities = []
        
        try:
            # GitHub GraphQL API for security advisories
            graphql_query = """
            query {
                securityAdvisories(first: 50, orderBy: {field: UPDATED_AT, direction: DESC}) {
                    nodes {
                        ghsaId
                        summary
                        description
                        severity
                        publishedAt
                        updatedAt
                        permalink
                        cvss {
                            score
                            vectorString
                        }
                        references {
                            url
                        }
                        vulnerabilities(first: 10) {
                            nodes {
                                package {
                                    name
                                    ecosystem
                                }
                                severity
                                firstPatchedVersion {
                                    identifier
                                }
                            }
                        }
                    }
                }
            }
            """
            
            headers = {"Content-Type": "application/json"}
            if settings.GITHUB_TOKEN:
                headers["Authorization"] = f"Bearer {settings.GITHUB_TOKEN}"
            
            payload = {"query": graphql_query}
            
            async with self.session.post(
                "https://api.github.com/graphql", 
                json=payload, 
                headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    advisories = data.get("data", {}).get("securityAdvisories", {}).get("nodes", [])
                    
                    for advisory in advisories:
                        vuln_data = await self.parse_github_advisory(advisory)
                        if vuln_data and await self.is_ai_related(vuln_data):
                            vulnerabilities.append(vuln_data)
                    
                    logger.info(f"Found {len(advisories)} GitHub advisories")
                else:
                    logger.warning(f"GitHub API returned {response.status}")
                    
        except Exception as e:
            logger.error(f"Error checking GitHub advisories: {e}")
        
        return vulnerabilities

    async def check_rss_feed(self, source: VulnSource) -> List[Dict]:
        """Check RSS feeds for security alerts"""
        logger.info(f"Checking RSS feed: {source.name}")
        vulnerabilities = []
        
        try:
            async with self.session.get(source.url) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    for entry in feed.entries[:20]:  # Check last 20 entries
                        # Check if entry is recent (last 7 days)
                        entry_date = datetime.now()
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            entry_date = datetime(*entry.published_parsed[:6])
                        
                        if (datetime.now() - entry_date).days <= 7:
                            vuln_data = await self.parse_rss_entry(entry, source)
                            if vuln_data and await self.is_ai_related(vuln_data):
                                vulnerabilities.append(vuln_data)
                    
                    logger.info(f"Found {len(feed.entries)} RSS entries from {source.name}")
                else:
                    logger.warning(f"RSS feed {source.name} returned {response.status}")
                    
        except Exception as e:
            logger.error(f"Error checking RSS feed {source.name}: {e}")
        
        return vulnerabilities

    async def parse_nvd_vulnerability(self, cve_item: Dict) -> Optional[Dict]:
        """Parse NVD CVE item into our vulnerability format"""
        try:
            cve = cve_item.get("cve", {})
            cve_id = cve.get("id")
            
            descriptions = cve.get("descriptions", [])
            description = next((d["value"] for d in descriptions if d.get("lang") == "en"), "")
            
            # Get CVSS score
            metrics = cve.get("metrics", {})
            cvss_score = 0.0
            cvss_vector = ""
            
            if "cvssMetricV31" in metrics:
                cvss_data = metrics["cvssMetricV31"][0]["cvssData"]
                cvss_score = cvss_data.get("baseScore", 0.0)
                cvss_vector = cvss_data.get("vectorString", "")
            elif "cvssMetricV2" in metrics:
                cvss_data = metrics["cvssMetricV2"][0]["cvssData"]
                cvss_score = cvss_data.get("baseScore", 0.0)
                cvss_vector = cvss_data.get("vectorString", "")
            
            # Map CVSS to severity
            if cvss_score >= 9.0:
                severity = SeverityEnum.CRITICAL
            elif cvss_score >= 7.0:
                severity = SeverityEnum.HIGH
            elif cvss_score >= 4.0:
                severity = SeverityEnum.MEDIUM
            else:
                severity = SeverityEnum.LOW
            
            # Get references
            references = []
            ref_data = cve.get("references", [])
            for ref in ref_data:
                if ref.get("url"):
                    references.append(ref["url"])
            
            return {
                "vulnerability_id": cve_id,
                "cve_id": cve_id,
                "title": f"CVE {cve_id}",
                "description": description,
                "severity": severity,
                "cvss_score": cvss_score,
                "cvss_vector": cvss_vector,
                "discovery_date": datetime.now(),
                "references": references,
                "source": "nvd",
                "raw_data": cve_item
            }
            
        except Exception as e:
            logger.error(f"Error parsing NVD vulnerability: {e}")
            return None

    async def parse_github_advisory(self, advisory: Dict) -> Optional[Dict]:
        """Parse GitHub security advisory"""
        try:
            ghsa_id = advisory.get("ghsaId")
            
            # Map GitHub severity to our enum
            gh_severity = advisory.get("severity", "MODERATE").upper()
            severity_map = {
                "CRITICAL": SeverityEnum.CRITICAL,
                "HIGH": SeverityEnum.HIGH,
                "MODERATE": SeverityEnum.MEDIUM,
                "LOW": SeverityEnum.LOW
            }
            severity = severity_map.get(gh_severity, SeverityEnum.MEDIUM)
            
            cvss_score = 0.0
            cvss_vector = ""
            if advisory.get("cvss"):
                cvss_score = advisory["cvss"].get("score", 0.0)
                cvss_vector = advisory["cvss"].get("vectorString", "")
            
            references = [advisory.get("permalink", "")]
            if advisory.get("references"):
                references.extend([ref.get("url") for ref in advisory["references"] if ref.get("url")])
            
            return {
                "vulnerability_id": ghsa_id,
                "title": advisory.get("summary", f"GitHub Advisory {ghsa_id}"),
                "description": advisory.get("description", ""),
                "severity": severity,
                "cvss_score": cvss_score,
                "cvss_vector": cvss_vector,
                "discovery_date": datetime.fromisoformat(advisory.get("publishedAt", "").replace("Z", "+00:00")),
                "references": references,
                "source": "github",
                "raw_data": advisory
            }
            
        except Exception as e:
            logger.error(f"Error parsing GitHub advisory: {e}")
            return None

    async def parse_rss_entry(self, entry: Any, source: VulnSource) -> Optional[Dict]:
        """Parse RSS feed entry"""
        try:
            title = getattr(entry, 'title', 'Unknown')
            description = getattr(entry, 'description', '') or getattr(entry, 'summary', '')
            link = getattr(entry, 'link', '')
            
            # Generate a unique ID based on title and source
            vuln_id = f"RSS-{source.name}-{hash(title) % 10000:04d}"
            
            # Try to extract CVE IDs from title/description
            cve_pattern = r'CVE-\d{4}-\d{4,7}'
            cve_matches = re.findall(cve_pattern, f"{title} {description}")
            cve_id = cve_matches[0] if cve_matches else None
            
            # Basic severity assessment based on keywords
            text = f"{title} {description}".lower()
            if any(word in text for word in ["critical", "severe", "remote code execution", "rce"]):
                severity = SeverityEnum.CRITICAL
            elif any(word in text for word in ["high", "important", "privilege escalation"]):
                severity = SeverityEnum.HIGH
            elif any(word in text for word in ["medium", "moderate", "information disclosure"]):
                severity = SeverityEnum.MEDIUM
            else:
                severity = SeverityEnum.LOW
            
            return {
                "vulnerability_id": vuln_id,
                "cve_id": cve_id,
                "title": title,
                "description": description,
                "severity": severity,
                "cvss_score": 5.0,  # Default
                "discovery_date": datetime.now(),
                "references": [link] if link else [],
                "source": f"rss_{source.name.lower()}",
                "raw_data": {"rss_entry": entry.__dict__}
            }
            
        except Exception as e:
            logger.error(f"Error parsing RSS entry: {e}")
            return None

    async def is_ai_related(self, vuln_data: Dict) -> bool:
        """STRICT AI/coding tool filter - ONLY AI coding assistants and development tools"""
        try:
            text = f"{vuln_data.get('title', '')} {vuln_data.get('description', '')}".lower()
            
            # PRIMARY AI CODING TOOLS - Cursor gets TOP priority
            primary_ai_tools = [
                # CURSOR - Our main AI IDE (highest priority)
                "cursor", "cursor ide", "cursor.sh", "cursor editor", "cursor ai", "cursor assistant",
                # Other AI tools
                "github copilot", "copilot", "tabnine", "tab nine", "codeium", 
                "amazon codewhisperer", "codewhisperer", "sourcegraph cody", "cody ai", 
                "jetbrains ai assistant", "jetbrains ai", "replit ghostwriter", "ghostwriter", 
                "intellicode", "visual studio intellicode", "deepcode", "snyk deepcode", 
                "ai code review", "ai pair programming"
            ]
            
            # Check for primary AI tools first
            for tool in primary_ai_tools:
                if tool in text:
                    logger.info(f"AI-related vulnerability found: {vuln_data.get('vulnerability_id')}")
                    return True
            
            # SECONDARY AI CODING PATTERNS - Require multiple indicators
            ai_coding_patterns = [
                # AI + Code patterns
                ("ai" in text and "code completion" in text),
                ("ai" in text and "code generation" in text), 
                ("ai" in text and "code suggestion" in text),
                ("ai" in text and "code assistant" in text),
                ("ai" in text and "autocomplete" in text),
                ("machine learning" in text and "code" in text),
                ("neural" in text and "code" in text),
                ("llm" in text and ("code" in text or "programming" in text)),
                ("large language model" in text and "code" in text),
                ("prompt injection" in text and ("code" in text or "ai" in text)),
                ("transformer" in text and "code" in text)
            ]
            
            # IDE/Editor specific AI features - Cursor prioritized
            ide_ai_patterns = [
                # Cursor IDE patterns get priority
                ("cursor" in text and ("ide" in text or "editor" in text)),
                ("cursor" in text and "extension" in text),
                # Other IDE AI patterns
                ("vscode" in text and "ai" in text),
                ("visual studio code" in text and "ai" in text),
                ("jetbrains" in text and "ai" in text),
                ("intellij" in text and "ai" in text),
                ("pycharm" in text and "ai" in text),
                ("webstorm" in text and "ai" in text)
            ]
            
            # Check secondary AI coding patterns
            if any(ai_coding_patterns):
                logger.info(f"AI coding pattern detected: {vuln_data.get('vulnerability_id')}")
                return True
                
            # Check IDE-specific AI patterns
            if any(ide_ai_patterns):
                logger.info(f"IDE AI feature detected: {vuln_data.get('vulnerability_id')}")
                return True
            
            # STRICT EXCLUSIONS - Reject anything not specifically AI coding related
            exclusion_patterns = [
                "networking", "hardware", "kernel", "driver", "firmware", "router",
                "database", "web server", "apache", "nginx", "mysql", "postgresql",
                "operating system", "windows", "linux", "macos", "android", "ios",
                "browser", "chrome", "firefox", "safari", "internet explorer",
                "office", "word", "excel", "powerpoint", "outlook",
                "general software", "library vulnerability", "protocol",
                "memory corruption", "buffer overflow", "privilege escalation"
            ]
            
            # If it contains exclusion patterns but no AI indicators, reject it
            has_exclusions = any(exclusion in text for exclusion in exclusion_patterns)
            if has_exclusions:
                return False
            
            return False
            
        except Exception as e:
            logger.error(f"Error in AI relevance check: {e}")
            return False

    async def classify_affected_tools(self, vuln_data: Dict) -> List[str]:
        """Determine which AI tools are affected by this vulnerability"""
        affected_tools = []
        text = f"{vuln_data.get('title', '')} {vuln_data.get('description', '')}".lower()
        
        tool_indicators = {
            "github_copilot": ["copilot", "github copilot", "vscode", "visual studio code"],
            "cursor": ["cursor", "cursor editor", "cursor ide"],
            "tabnine": ["tabnine", "tab nine"],
            "codeium": ["codeium"],
            "jetbrains_ai_assistant": ["jetbrains", "intellij", "pycharm", "webstorm", "idea"],
            "amazon_codewhisperer": ["codewhisperer", "code whisperer", "aws toolkit"],
            "sourcegraph_cody": ["sourcegraph", "cody"],
            "replit_ghostwriter": ["replit", "ghostwriter"]
        }
        
        for tool, indicators in tool_indicators.items():
            if any(indicator in text for indicator in indicators):
                affected_tools.append(tool)
        
        # If no specific tools identified but it's AI-related, assume broad impact
        if not affected_tools and await self.is_ai_related(vuln_data):
            # Check for platform-wide vulnerabilities
            if any(platform in text for platform in ["vscode", "visual studio", "electron", "node.js", "javascript"]):
                affected_tools = ["github_copilot", "cursor", "codeium"]
            elif any(java_term in text for java_term in ["java", "jvm", "log4j"]):
                affected_tools = ["jetbrains_ai_assistant"]
        
        return affected_tools

    async def save_vulnerability(self, vuln_data: Dict) -> bool:
        """Save new vulnerability to database"""
        try:
            with SessionLocal() as db:
                # Check if vulnerability already exists
                existing = db.query(Vulnerability).filter(
                    Vulnerability.vulnerability_id == vuln_data["vulnerability_id"]
                ).first()
                
                if existing:
                    logger.info(f"Vulnerability {vuln_data['vulnerability_id']} already exists")
                    return False
                
                # Determine affected tools
                affected_tool_names = await self.classify_affected_tools(vuln_data)
                
                # Create vulnerability object
                vuln = Vulnerability(
                    vulnerability_id=vuln_data["vulnerability_id"],
                    cve_id=vuln_data.get("cve_id"),
                    title=vuln_data["title"],
                    description=vuln_data["description"],
                    severity=vuln_data["severity"],
                    cvss_score=vuln_data["cvss_score"],
                    cvss_vector=vuln_data.get("cvss_vector"),
                    discovery_date=vuln_data["discovery_date"],
                    references=vuln_data["references"],
                    source=vuln_data["source"],
                    attack_vectors=[AttackVectorEnum.INJECTION],  # Default
                    patch_status=PatchStatusEnum.UNPATCHED,
                    tags=["automated-discovery"],
                    confidence_score=0.8
                )
                
                # Associate with affected tools
                tools_map = {tool.name: tool for tool in db.query(AITool).all()}
                for tool_name in affected_tool_names:
                    if tool_name in tools_map:
                        vuln.affected_tools.append(tools_map[tool_name])
                
                db.add(vuln)
                db.commit()
                
                logger.info(f"Saved new vulnerability: {vuln_data['vulnerability_id']}")
                return True
                
        except Exception as e:
            logger.error(f"Error saving vulnerability: {e}")
            return False

    async def check_json_api(self, source: VulnSource) -> List[Dict]:
        """Check JSON API endpoints for vulnerabilities"""
        logger.info(f"Checking JSON API: {source.name}")
        vulnerabilities = []
        
        try:
            headers = {'User-Agent': 'Kirin-VulnDB/1.0 (Security Research)'}
            
            if "vulners.com" in source.url:
                # Special handling for Vulners API
                for keyword in source.keywords[:5]:  # Limit to avoid rate limits
                    params = {
                        "query": f"title:{keyword}",
                        "limit": 10,
                        "sort": "published",
                        "order": "desc"
                    }
                    async with self.session.get(source.url, params=params, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            items = data.get("data", {}).get("search", [])
                            for item in items[:10]:
                                vuln_data = await self.parse_vulners_item(item)
                                if vuln_data and await self.is_ai_related(vuln_data):
                                    vulnerabilities.append(vuln_data)
                    await asyncio.sleep(1)  # Rate limiting
            else:
                # Generic JSON API handling
                async with self.session.get(source.url, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        # Handle different JSON structures
                        if isinstance(data, list):
                            items = data
                        elif "data" in data:
                            items = data["data"]
                        else:
                            items = [data]
                        
                        for item in items[:20]:
                            vuln_data = await self.parse_generic_json(item, source)
                            if vuln_data and await self.is_ai_related(vuln_data):
                                vulnerabilities.append(vuln_data)
                    else:
                        logger.warning(f"JSON API {source.name} returned {response.status}")
                        
        except Exception as e:
            logger.error(f"Error checking JSON API {source.name}: {e}")
        
        return vulnerabilities

    async def check_csv_download(self, source: VulnSource) -> List[Dict]:
        """Check CSV download sources for vulnerabilities"""
        logger.info(f"Checking CSV download: {source.name}")
        vulnerabilities = []
        
        try:
            # For now, skip CSV downloads to avoid heavy processing
            # This would require downloading large files and parsing
            logger.info(f"Skipping CSV download {source.name} - requires special handling")
                        
        except Exception as e:
            logger.error(f"Error checking CSV download {source.name}: {e}")
        
        return vulnerabilities

    async def check_web_scrape(self, source: VulnSource) -> List[Dict]:
        """Check web scraping sources for vulnerabilities"""
        logger.info(f"Checking web scrape: {source.name}")
        vulnerabilities = []
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; Kirin-VulnDB/1.0; Security Research)'
            }
            
            async with self.session.get(source.url, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # Basic HTML parsing for security advisories
                    import re
                    
                    # Look for CVE patterns
                    cve_pattern = r'CVE-\d{4}-\d{4,}'
                    cves = re.findall(cve_pattern, content)
                    
                    for cve in cves[:10]:  # Limit results
                        # Create basic vulnerability data
                        vuln_data = {
                            "vulnerability_id": cve,
                            "cve_id": cve,
                            "title": f"Web Scraped {cve}",
                            "description": f"Vulnerability {cve} found via web scraping from {source.name}",
                            "severity": "MEDIUM",  # Default
                            "cvss_score": 5.0,
                            "patch_status": "unpatched",
                            "attack_vectors": ["unknown"],
                            "references": [source.url],
                            "source": source.name,
                            "confidence_score": 0.5  # Lower confidence for scraped data
                        }
                        
                        if await self.is_ai_related(vuln_data):
                            vulnerabilities.append(vuln_data)
                else:
                    logger.warning(f"Web scrape {source.name} returned {response.status}")
                        
        except Exception as e:
            logger.error(f"Error checking web scrape {source.name}: {e}")
        
        return vulnerabilities

    async def parse_vulners_item(self, item: Dict) -> Optional[Dict]:
        """Parse Vulners API item into vulnerability format"""
        try:
            return {
                "vulnerability_id": item.get("id", ""),
                "cve_id": item.get("cvelist", [""])[0] if item.get("cvelist") else None,
                "title": item.get("title", ""),
                "description": item.get("description", ""),
                "severity": self.map_cvss_to_severity(item.get("cvss", {}).get("score", 0)),
                "cvss_score": item.get("cvss", {}).get("score", 0),
                "patch_status": "unpatched",
                "attack_vectors": ["unknown"],
                "references": [item.get("href", "")],
                "source": "VULNERS",
                "confidence_score": 0.8
            }
        except Exception as e:
            logger.error(f"Error parsing Vulners item: {e}")
            return None

    async def parse_generic_json(self, item: Dict, source: VulnSource) -> Optional[Dict]:
        """Parse generic JSON item into vulnerability format"""
        try:
            # Basic mapping for generic JSON APIs
            return {
                "vulnerability_id": item.get("id", f"{source.name}-{hash(str(item))}"),
                "cve_id": item.get("cve") or item.get("cve_id"),
                "title": item.get("title") or item.get("name") or item.get("summary", ""),
                "description": item.get("description") or item.get("summary", ""),
                "severity": "MEDIUM",  # Default
                "cvss_score": item.get("cvss_score", 5.0),
                "patch_status": "unpatched",
                "attack_vectors": ["unknown"],
                "references": [item.get("url", source.url)],
                "source": source.name,
                "confidence_score": 0.6
            }
        except Exception as e:
            logger.error(f"Error parsing generic JSON: {e}")
            return None

    def map_cvss_to_severity(self, cvss_score: float) -> str:
        """Map CVSS score to severity level"""
        if cvss_score >= 9.0:
            return "CRITICAL"
        elif cvss_score >= 7.0:
            return "HIGH"
        elif cvss_score >= 4.0:
            return "MEDIUM"
        else:
            return "LOW"

    async def run_discovery_cycle(self) -> Dict[str, int]:
        """Run one complete discovery cycle"""
        logger.info("Starting vulnerability discovery cycle...")
        stats = {"total_found": 0, "new_vulnerabilities": 0, "sources_checked": 0}
        
        for source in self.sources:
            try:
                logger.info(f"Checking source: {source.name}")
                vulnerabilities = []
                
                if source.source_type == "nvd_api":
                    vulnerabilities = await self.check_nvd_api(source)
                elif source.source_type == "github_api":
                    vulnerabilities = await self.check_github_advisories(source)
                elif source.source_type == "rss":
                    vulnerabilities = await self.check_rss_feed(source)
                elif source.source_type == "json_api":
                    vulnerabilities = await self.check_json_api(source)
                elif source.source_type == "csv_download":
                    vulnerabilities = await self.check_csv_download(source)
                elif source.source_type == "web_scrape":
                    vulnerabilities = await self.check_web_scrape(source)
                
                stats["sources_checked"] += 1
                stats["total_found"] += len(vulnerabilities)
                
                # Save new vulnerabilities
                for vuln_data in vulnerabilities:
                    if await self.save_vulnerability(vuln_data):
                        stats["new_vulnerabilities"] += 1
                
                # Rate limiting between sources
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error checking source {source.name}: {e}")
        
        logger.info(f"Discovery cycle complete: {stats}")
        return stats

# Monitoring scheduler
class VulnerabilityScheduler:
    """Handles scheduled vulnerability monitoring"""
    
    def __init__(self):
        self.monitor = None
        self.running = False
    
    async def start_monitoring(self):
        """Start the 6-hour monitoring cycle"""
        logger.info("Starting 6-hour vulnerability monitoring...")
        self.running = True
        
        while self.running:
            try:
                async with VulnerabilityMonitor() as monitor:
                    stats = await monitor.run_discovery_cycle()
                    logger.info(f"Monitoring cycle completed: {stats}")
                
                # Wait 6 hours before next check
                logger.info("Waiting 6 hours for next vulnerability scan...")
                await asyncio.sleep(6 * 60 * 60)  # 6 hours
                
            except Exception as e:
                logger.error(f"Error in monitoring cycle: {e}")
                # Wait 30 minutes before retry on error
                await asyncio.sleep(30 * 60)
    
    async def stop_monitoring(self):
        """Stop the monitoring cycle"""
        logger.info("Stopping vulnerability monitoring...")
        self.running = False

# Global scheduler instance
vulnerability_scheduler = VulnerabilityScheduler()

async def start_vulnerability_monitoring():
    """Start the vulnerability monitoring system"""
    await vulnerability_scheduler.start_monitoring()

async def stop_vulnerability_monitoring():
    """Stop the vulnerability monitoring system"""
    await vulnerability_scheduler.stop_monitoring()