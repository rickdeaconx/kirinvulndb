"""
AI-Powered Vulnerability Analysis Service
Analyzes submitted vulnerability data and enhances it with AI insights
Copyright Â© 2025 Rick Deacon, Knostic AI - https://knostic.ai
"""

import re
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)

@dataclass
class VulnerabilityAnalysis:
    """Result of AI vulnerability analysis"""
    enhanced_title: str
    enhanced_description: str
    severity: str
    cvss_score: float
    attack_vectors: List[str]
    affected_tools: List[str]
    patch_status: str
    confidence_score: float
    technical_details: str
    remediation_suggestions: List[str]
    risk_assessment: str
    ai_analysis_notes: str

class AIVulnerabilityAnalyzer:
    """AI-powered vulnerability analysis system"""
    
    def __init__(self):
        self.ai_tools_database = {
            # Cursor is the PRIMARY AI IDE - highest priority
            "cursor": ["cursor", "cursor.sh", "cursor ide", "cursor editor", "cursor ai", "cursor assistant"],
            "github_copilot": ["copilot", "github copilot", "gh copilot", "copilot extension"],
            "tabnine": ["tabnine", "tab nine", "tabnine ai"],
            "codeium": ["codeium", "codeium ai", "codeium extension"],
            "amazon_codewhisperer": ["codewhisperer", "amazon codewhisperer", "aws codewhisperer"],
            "sourcegraph_cody": ["cody", "sourcegraph cody", "cody ai"],
            "jetbrains_ai_assistant": ["jetbrains ai", "intellij ai", "jetbrains assistant"],
            "replit_ghostwriter": ["ghostwriter", "replit ghostwriter", "repl.it ai"],
            "vscode": ["vscode", "visual studio code", "vs code"],
            "deepcode": ["deepcode", "deepcode ai", "snyk deepcode"],
            "intellicode": ["intellicode", "visual studio intellicode"]
        }
        
        self.severity_keywords = {
            "CRITICAL": ["critical", "rce", "remote code execution", "arbitrary code", "privilege escalation", "authentication bypass"],
            "HIGH": ["high", "injection", "xss", "sql injection", "path traversal", "information disclosure"],
            "MEDIUM": ["medium", "csrf", "directory traversal", "weak encryption", "session fixation"],
            "LOW": ["low", "information leak", "weak password", "configuration issue", "denial of service"]
        }
        
        self.attack_vector_patterns = {
            "injection": ["injection", "sql injection", "command injection", "ldap injection", "xpath injection"],
            "prompt_injection": ["prompt injection", "prompt manipulation", "ai prompt", "llm injection"],
            "xss": ["xss", "cross-site scripting", "reflected xss", "stored xss", "dom xss"],
            "rce": ["rce", "remote code execution", "arbitrary code", "code execution"],
            "privilege_escalation": ["privilege escalation", "privesc", "elevation", "root access"],
            "data_exfiltration": ["data exfiltration", "information disclosure", "data leak", "sensitive data"],
            "model_poisoning": ["model poisoning", "training data", "adversarial", "backdoor"],
            "backdoor": ["backdoor", "malicious model", "trojan", "hidden functionality"]
        }

    async def analyze_vulnerability(self, vulnerability_data: Dict[str, Any]) -> VulnerabilityAnalysis:
        """
        Analyze a submitted vulnerability using AI-powered techniques
        """
        logger.info(f"Starting AI analysis of vulnerability: {vulnerability_data.get('title', 'Unknown')}")
        
        # Extract and clean input data
        title = vulnerability_data.get("title", "").strip()
        description = vulnerability_data.get("description", "").strip()
        cve_id = vulnerability_data.get("cve_id", "").strip()
        references = vulnerability_data.get("references", [])
        
        # Combine all text for analysis
        full_text = f"{title} {description} {' '.join(references)}".lower()
        
        # AI-powered analysis
        enhanced_title = await self._enhance_title(title, description, cve_id)
        enhanced_description = await self._enhance_description(description, full_text)
        severity = await self._analyze_severity(full_text)
        cvss_score = await self._estimate_cvss_score(severity, full_text)
        attack_vectors = await self._identify_attack_vectors(full_text)
        affected_tools = await self._identify_affected_tools(full_text)
        patch_status = await self._determine_patch_status(full_text, references)
        technical_details = await self._extract_technical_details(description, full_text)
        remediation_suggestions = await self._generate_remediation_suggestions(attack_vectors, affected_tools, severity)
        risk_assessment = await self._assess_risk(severity, attack_vectors, affected_tools)
        
        # Calculate confidence score based on available data
        confidence_score = await self._calculate_confidence_score(vulnerability_data, full_text)
        
        # Generate AI analysis notes
        ai_analysis_notes = await self._generate_analysis_notes(
            enhanced_title, severity, attack_vectors, affected_tools, confidence_score
        )
        
        analysis = VulnerabilityAnalysis(
            enhanced_title=enhanced_title,
            enhanced_description=enhanced_description,
            severity=severity,
            cvss_score=cvss_score,
            attack_vectors=attack_vectors,
            affected_tools=affected_tools,
            patch_status=patch_status,
            confidence_score=confidence_score,
            technical_details=technical_details,
            remediation_suggestions=remediation_suggestions,
            risk_assessment=risk_assessment,
            ai_analysis_notes=ai_analysis_notes
        )
        
        logger.info(f"AI analysis completed: {severity} severity, {len(attack_vectors)} attack vectors, confidence: {confidence_score}")
        return analysis

    async def _enhance_title(self, title: str, description: str, cve_id: str) -> str:
        """Enhance vulnerability title with AI analysis"""
        if not title or title == cve_id:
            # Generate title from description
            if "prompt injection" in description.lower():
                return f"AI Prompt Injection Vulnerability - {cve_id}"
            elif "code execution" in description.lower():
                return f"Remote Code Execution Vulnerability - {cve_id}"
            elif "privilege escalation" in description.lower():
                return f"Privilege Escalation Vulnerability - {cve_id}"
            else:
                return f"AI Security Vulnerability - {cve_id}"
        
        # Enhance existing title
        enhanced = title
        if cve_id and cve_id not in enhanced:
            enhanced = f"{enhanced} ({cve_id})"
            
        return enhanced

    async def _enhance_description(self, description: str, full_text: str) -> str:
        """Enhance vulnerability description with AI insights"""
        if not description:
            return "AI-powered vulnerability analysis detected security issues requiring investigation."
        
        enhanced = description
        
        # Add context about AI tools if detected
        ai_tools_found = []
        for tool, keywords in self.ai_tools_database.items():
            if any(keyword in full_text for keyword in keywords):
                ai_tools_found.append(tool.replace("_", " ").title())
        
        if ai_tools_found:
            enhanced += f"\n\nAI Tools Potentially Affected: {', '.join(ai_tools_found)}"
        
        # Add severity context
        if "critical" in full_text or "rce" in full_text:
            enhanced += "\n\nSeverity Assessment: This vulnerability poses a critical risk to AI development environments."
        
        return enhanced

    async def _analyze_severity(self, text: str) -> str:
        """Analyze severity using keyword matching and context"""
        severity_scores = {}
        
        for severity, keywords in self.severity_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            severity_scores[severity] = score
        
        # Default to MEDIUM if no clear indicators
        if not severity_scores or max(severity_scores.values()) == 0:
            return "MEDIUM"
        
        return max(severity_scores, key=severity_scores.get)

    async def _estimate_cvss_score(self, severity: str, text: str) -> float:
        """Estimate CVSS score based on severity and content analysis"""
        base_scores = {
            "CRITICAL": 9.0,
            "HIGH": 7.5,
            "MEDIUM": 5.0,
            "LOW": 2.5
        }
        
        base_score = base_scores.get(severity, 5.0)
        
        # Adjust based on specific indicators
        if "remote" in text and "code execution" in text:
            base_score = min(10.0, base_score + 1.0)
        elif "authentication bypass" in text:
            base_score = min(10.0, base_score + 0.5)
        elif "local" in text and "privilege" in text:
            base_score = max(1.0, base_score - 1.0)
        
        return round(base_score, 1)

    async def _identify_attack_vectors(self, text: str) -> List[str]:
        """Identify attack vectors using pattern matching"""
        vectors = []
        
        for vector, patterns in self.attack_vector_patterns.items():
            if any(pattern in text for pattern in patterns):
                vectors.append(vector)
        
        # Default to generic injection if AI-related
        if not vectors and any(ai_term in text for ai_term in ["ai", "llm", "prompt", "model"]):
            vectors.append("prompt_injection")
        elif not vectors:
            vectors.append("injection")  # Default fallback
        
        return vectors

    async def _identify_affected_tools(self, text: str) -> List[str]:
        """Identify affected AI tools using database matching"""
        affected = []
        
        for tool, keywords in self.ai_tools_database.items():
            if any(keyword in text for keyword in keywords):
                affected.append(tool)
        
        # If no specific tools found but AI-related, assume Cursor (our primary IDE)
        if not affected and any(ai_term in text for ai_term in ["ai", "llm", "artificial intelligence", "machine learning"]):
            affected = ["cursor"]  # Default to Cursor as primary AI IDE
        
        return affected

    async def _determine_patch_status(self, text: str, references: List[str]) -> str:
        """Determine patch status from text and references"""
        if any(patch_term in text for patch_term in ["fixed in", "patched", "update", "version"]):
            return "patch_available"
        elif any(patch_term in text for patch_term in ["no fix", "wont fix", "will not fix"]):
            return "wont_fix"
        elif any(patch_term in text for patch_term in ["fixed", "resolved", "mitigated"]):
            return "patched"
        else:
            return "unpatched"

    async def _extract_technical_details(self, description: str, text: str) -> str:
        """Extract and enhance technical details"""
        technical_details = []
        
        # Look for technical patterns
        if "cve-" in text:
            cve_matches = re.findall(r'CVE-\d{4}-\d{4,}', text.upper())
            if cve_matches:
                technical_details.append(f"Related CVEs: {', '.join(set(cve_matches))}")
        
        # Look for version information
        version_matches = re.findall(r'version\s+([0-9.]+)', text)
        if version_matches:
            technical_details.append(f"Affected versions: {', '.join(set(version_matches))}")
        
        # Add description if no technical details found
        if not technical_details and description:
            technical_details.append(description)
        
        return "; ".join(technical_details) if technical_details else "Technical analysis pending further investigation."

    async def _generate_remediation_suggestions(self, attack_vectors: List[str], affected_tools: List[str], severity: str) -> List[str]:
        """Generate AI-powered remediation suggestions"""
        suggestions = []
        
        # Vector-specific suggestions
        if "prompt_injection" in attack_vectors:
            suggestions.extend([
                "Implement input validation and sanitization for AI prompts",
                "Deploy prompt injection detection mechanisms",
                "Restrict AI model access to sensitive operations",
                "Enable AI safety guardrails and content filtering"
            ])
        
        if "rce" in attack_vectors:
            suggestions.extend([
                "Update affected tools to latest patched versions",
                "Implement network segmentation and access controls",
                "Deploy endpoint detection and response (EDR) solutions",
                "Regular security scanning of development environments"
            ])
        
        if "data_exfiltration" in attack_vectors:
            suggestions.extend([
                "Implement data loss prevention (DLP) controls",
                "Monitor and log AI tool data access patterns",
                "Encrypt sensitive data at rest and in transit",
                "Regular audit of AI tool permissions"
            ])
        
        # Tool-specific suggestions - Cursor gets priority
        if "cursor" in affected_tools:
            suggestions.extend([
                "Update Cursor IDE to the latest version immediately",
                "Review Cursor IDE security settings and AI assistant permissions", 
                "Check Cursor extension marketplace for security updates",
                "Configure Cursor's AI safety settings and content filters"
            ])
        
        if "github_copilot" in affected_tools:
            suggestions.append("Update GitHub Copilot and review code suggestion policies")
        
        # Severity-based suggestions
        if severity == "CRITICAL":
            suggestions.insert(0, "IMMEDIATE ACTION REQUIRED: Disable affected tools until patched")
        
        return list(set(suggestions))  # Remove duplicates

    async def _assess_risk(self, severity: str, attack_vectors: List[str], affected_tools: List[str]) -> str:
        """Generate comprehensive risk assessment"""
        risk_factors = []
        
        # Severity impact
        severity_impact = {
            "CRITICAL": "Extreme risk of complete system compromise",
            "HIGH": "High risk of significant security impact",
            "MEDIUM": "Moderate risk requiring attention",
            "LOW": "Low risk but should be addressed"
        }
        risk_factors.append(severity_impact.get(severity, "Risk assessment pending"))
        
        # Vector-specific risks
        if "rce" in attack_vectors:
            risk_factors.append("Remote code execution enables full system control")
        if "prompt_injection" in attack_vectors:
            risk_factors.append("AI prompt manipulation can lead to data exposure")
        if "data_exfiltration" in attack_vectors:
            risk_factors.append("Sensitive code and data may be compromised")
        
        # Tool impact
        if len(affected_tools) > 2:
            risk_factors.append("Multiple AI tools affected increases attack surface")
        
        return " | ".join(risk_factors)

    async def _calculate_confidence_score(self, vulnerability_data: Dict, text: str) -> float:
        """Calculate confidence score based on data quality"""
        score = 0.5  # Base score
        
        # Data quality factors
        if vulnerability_data.get("cve_id"):
            score += 0.2
        if vulnerability_data.get("references") and len(vulnerability_data["references"]) > 0:
            score += 0.2
        if len(vulnerability_data.get("description", "")) > 100:
            score += 0.1
        
        # AI detection confidence
        ai_terms = ["ai", "llm", "prompt", "model", "copilot", "cursor", "tabnine"]
        ai_term_count = sum(1 for term in ai_terms if term in text)
        if ai_term_count >= 2:
            score += 0.1
        elif ai_term_count >= 1:
            score += 0.05
        
        return min(1.0, round(score, 2))

    async def _generate_analysis_notes(self, title: str, severity: str, attack_vectors: List[str], 
                                     affected_tools: List[str], confidence_score: float) -> str:
        """Generate AI analysis summary notes"""
        notes = []
        
        notes.append(f"AI Analysis Summary for: {title}")
        notes.append(f"Severity Classification: {severity} (Confidence: {confidence_score})")
        
        if attack_vectors:
            notes.append(f"Identified Attack Vectors: {', '.join(attack_vectors)}")
        
        if affected_tools:
            notes.append(f"AI Tools at Risk: {', '.join(affected_tools)}")
        else:
            notes.append("General AI development environment vulnerability")
        
        notes.append(f"Analysis completed at {datetime.utcnow().isoformat()}Z")
        notes.append("Generated by Knostic AI Security Analysis Engine")
        
        return " | ".join(notes)